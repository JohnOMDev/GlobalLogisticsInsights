{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import sqlite3\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from .env into the environment\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the News Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.maritimelogisticsprofessional.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.implicitly_wait(5)\n",
    "browser.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wait for the button to be clickable\n",
    "    WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\"))\n",
    "    )\n",
    "    # Click the button once it is clickable\n",
    "    browser.find_element(By.XPATH, \"//button[text()='Ok']\").click()\n",
    "except (NoSuchElementException, TimeoutException):\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of latest news\n",
    "latest_news = browser.find_elements(By.CLASS_NAME, \"snippet-flex\")\n",
    "latest_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all URLs before navigating\n",
    "article_urls = [element.get_attribute('href') for element in latest_news if '/news/' in element.get_attribute('href')]\n",
    "# Premium version\n",
    "premium = False\n",
    "\n",
    "article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article\n",
    "article_link = latest_news[0].get_attribute('href')\n",
    "article_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article\n",
    "time.sleep(5)\n",
    "browser.get(article_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the title element and get the text\n",
    "title_element = browser.find_element(By.CSS_SELECTOR, \"h1[itemprop='name']\")\n",
    "article_title = title_element.text\n",
    "\n",
    "# Find the article body element and get all the paragraph texts\n",
    "article_body_element = browser.find_element(By.CSS_SELECTOR, \"div[property='articleBody']\")\n",
    "article_paragraphs = article_body_element.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "# Combine the text of all paragraphs to form the body text\n",
    "article_text = \" \".join(paragraph.text for paragraph in article_paragraphs)\n",
    "\n",
    "# Now you have the article's title and text\n",
    "print(\"Title:\", article_title)\n",
    "print(\"Article Text:\", article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all URLs before navigating\n",
    "urls = [element.get_attribute('href') for element in latest_news]\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Getting {url}\")\n",
    "    time.sleep(5)  # Consider using WebDriverWait here instead of time.sleep for better efficiency\n",
    "    browser.get(url)\n",
    "    # Now you can perform your scraping logic on each article page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing the Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords from JSON file\n",
    "with open('../json/keywords.json', 'r') as file:\n",
    "    keywords = json.load(file)\n",
    "\n",
    "def classify_article(article_text, keywords):\n",
    "    max_count = 0\n",
    "    max_category = \"Unclassified or Neutral\"\n",
    "\n",
    "    # Convert article text to lower case for comparison\n",
    "    article_text_lower = article_text.lower()\n",
    "    \n",
    "    # Check for the presence of each keyword in the article\n",
    "    for category, category_keywords in keywords.items():\n",
    "        count = sum(keyword in article_text_lower for keyword in category_keywords)\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_category = category\n",
    "\n",
    "    return max_category\n",
    "\n",
    "# Usage example:\n",
    "classification = classify_article(article_text, keywords)\n",
    "print(f\"The article is related to: {classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt= f\"\"\"\n",
    "Please summarize the key points of the article for a business audience in a concise paragraph, limiting the summary to no more than 350 characters. Then, in a separate paragraph of 500 characters, elaborate on the potential impact of the situation described in the article on maritime logistics, port operations, and supply chain management, specifically focusing on its implications for Latin America. Label this second paragraph 'Impacto en LATAM:' and ensure there is a clear separation between the two sections. Present your response in Spanish and format it as markdown text for clarity:\\n\\n{text}\n",
    "                \"\"\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "#summary = summarize_text(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"Get Premium for enabling AI-powered summary!\"\n",
    "location = \"Global / LATAM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Original Article, Title and Summary in DDBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishes a connection to the SQLite database\n",
    "def connect_to_db(db_path):\n",
    "    return sqlite3.connect(db_path)\n",
    "\n",
    "# Create a new table for storing only daily articles\n",
    "def create_daily_table(cursor, table_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        title TEXT,\n",
    "                        text TEXT,\n",
    "                        summary TEXT,\n",
    "                        classification TEXT,\n",
    "                        location TEXT\n",
    "                    );\"\"\")\n",
    "\n",
    "# Create a new table for storing daily links\n",
    "def create_daily_links_table(cursor, table_name, articles_table_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        article_id INTEGER,\n",
    "                        title TEXT,\n",
    "                        link TEXT,\n",
    "                        FOREIGN KEY (article_id) REFERENCES {articles_table_name}(id)\n",
    "                    );\"\"\")\n",
    "\n",
    "# Inserts an article into the daily table\n",
    "def insert_article_data(cursor, table_name, article_title, article_text, summary, classification, location):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if an article with the same title already exists\n",
    "    cursor.execute(f\"SELECT id FROM {table_name} WHERE title = ?\", (article_title,))\n",
    "    existing_article = cursor.fetchone()\n",
    "\n",
    "    if existing_article == None:\n",
    "        cursor.execute(f\"\"\"INSERT INTO {table_name} (title, text, summary, classification, location)\n",
    "                        VALUES (?, ?, ?, ?, ?);\"\"\",\n",
    "                    (article_title, article_text, summary, classification, location))\n",
    "        return cursor.lastrowid  # Return the ID of the new article\n",
    "    else:\n",
    "        return existing_article[0]  # Return the ID of the existing article\n",
    "\n",
    "# Inserts a link into the daily links table\n",
    "def insert_link_data(cursor, table_name, article_id, article_title, link):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if the link already exists for the given article\n",
    "    cursor.execute(f\"SELECT id FROM {table_name} WHERE article_id = ? AND link = ?\", (article_id, link))\n",
    "    existing_link = cursor.fetchone()\n",
    "\n",
    "    if existing_link is None:\n",
    "        # If the link does not exist for this article, insert it\n",
    "        cursor.execute(f\"\"\"INSERT INTO {table_name} (article_id, title, link)\n",
    "                           VALUES (?, ?, ?);\"\"\",\n",
    "                       (article_id, article_title, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDBB Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the SQLite database\n",
    "db_path = '../data/news/maritime_news.db'\n",
    "\n",
    "# Define table naming\n",
    "current_date = datetime.now().strftime(\"%m%d%Y\")\n",
    "\n",
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for today's date with news\n",
    "news_table_name = f\"news_{current_date}\"\n",
    "create_daily_table(cursor, news_table_name)\n",
    "\n",
    "# Create table for storing links\n",
    "links_table_name = f\"news_links_{current_date}\"\n",
    "create_daily_links_table(cursor, links_table_name, news_table_name)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDBB News/Links Input or Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert the article data into the table and get the article ID\n",
    "create_daily_table(cursor, news_table_name)\n",
    "article_id = insert_article_data(cursor, news_table_name, article_title, article_text, summary, classification, location)\n",
    "\n",
    "# Insert the link data into the links table\n",
    "insert_link_data(cursor, links_table_name, article_id, article_title, article_link)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a browser session\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.implicitly_wait(2)\n",
    "browser.get(URL)\n",
    "\n",
    "# Cookies button\n",
    "try:\n",
    "    # Wait for the button to be clickable\n",
    "    WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\"))\n",
    "    )\n",
    "    # Click the button once it is clickable\n",
    "    browser.find_element(By.XPATH, \"//button[text()='Ok']\").click()\n",
    "except (NoSuchElementException, TimeoutException):\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLOBAL LATEST NEWS\n",
    "\"\"\"\n",
    "\n",
    "# List of latest global news\n",
    "latest_news = browser.find_elements(By.CLASS_NAME, \"snippet-flex\")\n",
    "# Collect all URLs before navigating\n",
    "article_urls = [element.get_attribute('href') for element in latest_news]\n",
    "# Premium version\n",
    "premium = False\n",
    "\n",
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for article in article_urls:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get article\n",
    "    time.sleep(5)\n",
    "    browser.get(article)\n",
    "\n",
    "    # Find the title element and get the text\n",
    "    title_element = browser.find_element(By.CSS_SELECTOR, \"h1[itemprop='name']\")\n",
    "    article_title = title_element.text\n",
    "\n",
    "    # Find the article body element and get all the paragraph texts\n",
    "    article_body_element = browser.find_element(By.CSS_SELECTOR, \"div[property='articleBody']\")\n",
    "    article_paragraphs = article_body_element.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "    # Combine the text of all paragraphs to form the body text\n",
    "    article_text = \" \".join(paragraph.text for paragraph in article_paragraphs)\n",
    "\n",
    "    # AI-powered Summary\n",
    "    if premium:\n",
    "        summarize_text(article_text)\n",
    "    else:\n",
    "        # Summary not premium\n",
    "        summary = \"Get Premium for enabling AI-powered summary!\"\n",
    "    \n",
    "    # Article classification\n",
    "    classification = classify_article(article_text, keywords)\n",
    "    # Article location\n",
    "    location = \"Global\"\n",
    "    \n",
    "    # Insert the article data into the table and get the article ID\n",
    "    article_id = insert_article_data(cursor, news_table_name, article_title, article_text, summary, classification, location)\n",
    "    # Insert the link data into the links table\n",
    "    insert_link_data(cursor, links_table_name, article_id, article_title, article_link)\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "# Close browser and database connection\n",
    "browser.quit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.maritimelogisticsprofessional.com\"\n",
    "\n",
    "# Create a browser session\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.get(URL)\n",
    "\n",
    "# Define a wait variable with a timeout of 10 seconds\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "# Cookies button\n",
    "try:\n",
    "    # Wait for the button to be clickable and click it\n",
    "    cookie_button = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\")),\n",
    "        message=\"Cookie button not clickable.\"\n",
    "    )\n",
    "    cookie_button.click()\n",
    "except TimeoutException:\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.maritimeprofessional.com/south-america\n",
      "APM Terminals Callao Expansion Moves Forward\n",
      "APM Terminals Callao commenced works on stage 3A of the Multipurpose North Terminal Modernization Project in the Port of Callao this week. The $95 million, 100% private investment, exceeds contractual commitments in the addendum by almost 40%.Works to be carried out during 2024 include the construction of a battery of 12 vertical silos for clean grains. This will increase capacity from 25,000 to 85…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Venezuela Oil Exports Rising, but Shipping Delays Persist\n",
      "Venezuela's oil exports slightly increased in February to some 670,000 barrels per day (bpd), but ongoing shipping delays worsened a bottleneck of tankers waiting to load, according to documents and vessel monitoring data.State-run oil firm…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Tanker Backlog Grows in Venezuela as PDVSA Struggles to Deliver Oil\n",
      "A bottleneck of vessels waiting to load crude and fuel in Venezuela has increased in recent weeks as state-run oil firm PDVSA struggles to deliver cargoes on time, according to people familiar with the matter, documents and shipping data.PDVSA has sought to ramp up shipments this month…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Surging Drug Violence Has Uruguay Clamoring for DEA Help\n",
      "Uruguay's main port received two cargo scanners sixteen years ago to detect drugs and other suspicious loads. Unfortunately, during delivery one of them fell into the sea.Since then, cocaine shipments to Europe have surged through the port of Montevideo…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Panama Canal Toll Revenue Shrinking Due to Drought\n",
      "The Panama Canal's toll revenues have dipped by about $100 million per month since last October, the canal's administrator said on Wednesday, adding that if the trend continues reduced income from tolls could total some $700 million by around April…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Port of Sudeste to Deploy RightShip's Maritime Emissions Portal\n",
      "RightShip announced a partnership with Port of Sudeste, located in Itaguai, Rio de Janeiro, to start utilizing RightShip’s Maritime Emissions Portal (MEP) for the first time in the Latin American region.MEP is a digital solution that combines…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Panama Canal Reports No Traffic Increase Amid Red Sea Attacks\n",
      "The Panama Canal Authority said on Thursday it has not seen a notable traffic increase due to the situation in the Red Sea, where attacks by Yemen's Houthi group are forcing vessels to divert or switch their transponders off.The hostilities…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Konecranes Delivers Cranes and Forklifts to Brazil's Port of Santos\n",
      "Konecranes delivered seven forklifts and two gantry cranes to Eldorado Brasil’s new cellulose terminal in Port of Santos in Q3 2023, completing an order that also involved joint planning and consulting on the layout of the terminal to ensure…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "US LNG Exports Through Panama Canal Shrink, Asia-Europe Spreads Widen\n",
      "The number of U.S. liquefied natural gas (LNG) vessels transiting the Panama Canal to Asia halved in November compared with a year ago as Asia prices for the gas this week traded at their steepest premium to European prices in nearly two years…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Konecranes Delivers Cranes for Brazilian Port Expansion Project\n",
      "In August 2023, CMA TERMINALS Fortaleza received delivery of two Generation 6 ESP.8 Konecranes Gottwald Mobile Harbor Cranes for its terminal in northeastern Brazil. The cranes will be used to build local container handling operations as part of the first stage of a port expansion project…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "In Brazil's Amazon, Cargill Grains Ports Meet Local Resistance\n",
      "For centuries, riverside communities, including the 'quilombola' descendants of enslaved Africans who escaped from plantations and ranches, have shared Xingu Island in Brazil's Amazon Basin.Its inhabitants live in brightly painted wooden houses…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Panama Canal to Slash Booking Slots Due to Drought\n",
      "Daily ship crossings on the Panama Canal, one of the world's main maritime trade routes, will be further reduced over the coming months due to a severe drought, the authorities managing the canal said late on Monday.Booking slots will be cut to 25 per day between November 3 to November 6…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Fire-hit Paranagua Port Berth to Resume Ops Nov. 4\n",
      "Shipping agent Cargonave said on Tuesday Berth 201 on the West Corridor of Brazil's port of Paranagua would tentatively resume operations on Saturday, according to a note to clients based on information it said it received from the local port authority…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "Crowley to Launch LNG Bunkering at Panama Canal's Pacific Side\n",
      "Shipping and energy logistics company Crowley said Tuesday it is proceeding with plans to provide liquefied natural gas (LNG) bunkering services on the Pacific side of the Panama Canal, under the first permit issued by the Panama Maritime Authority (AMP) for the provision of such services…\n",
      "https://www.maritimeprofessional.com/south-america\n",
      "It's Crunch Time for Brazilian Ports\n",
      "Brazil’s commodities export hubs are strained with record volumes of soy, corn and sugar to be moved at a time of the year when rains start to increase in southern ports, according to traders, analysts and shipping data.Exporters are reporting…\n"
     ]
    }
   ],
   "source": [
    "# Filter for LATAM\n",
    "filter_news = '/south-america'\n",
    "# Wait for the category links to be present and iterate through them\n",
    "cat_links = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".cat-link\")))\n",
    "\n",
    "latam_link = None\n",
    "for cat_link_div in cat_links:\n",
    "    a_element = cat_link_div.find_element(By.TAG_NAME, 'a')\n",
    "    href = a_element.get_attribute('href')\n",
    "    if href.endswith(filter_news):\n",
    "        latam_link = href\n",
    "        break  # Exit the loop once we find the LATAM link\n",
    "\n",
    "# Check if the LATAM link was found before proceeding\n",
    "if latam_link:\n",
    "    # Now navigate to the LATAM link\n",
    "    browser.get(latam_link)\n",
    "\n",
    "    # Wait for snippets to be present after navigating to the LATAM page\n",
    "    snippets = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".snippet\")))\n",
    "\n",
    "    # Process each snippet\n",
    "    for snippet in snippets:\n",
    "        news_link = snippet.get_attribute('href')\n",
    "        news_title = snippet.find_element(By.TAG_NAME, 'h2').text\n",
    "        body_text = snippet.find_element(By.TAG_NAME, 'p').text\n",
    "\n",
    "        print(news_link)\n",
    "        print(news_title)\n",
    "        print(body_text)\n",
    "else:\n",
    "    print(\"LATAM link was not found\")\n",
    "\n",
    "browser.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-dsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
